# How to Install PySpark in Windows 11
## Step 1 - Install Java
1)  Spark runs on Java 8/11/17, Scala 2.12/2.13, Python 3.7+, and R 3.5+. Python 3.7 support is deprecated as of Spark 3.4.0. Java 8 prior to version 8u362 support is deprecated as of Spark 3.4.0. When using the Scala API, it is necessary for applications to use the same version of Scala that Spark was compiled for. For example, when using Scala 2.13, use Spark compiled for 2.13, and compile code/applications for Scala 2.13 as well.
2)  Download JDK from Orcale Downloads page (you may need to signup and login to Oracle to download JDKs) - https://www.oracle.com/java/technologies/downloads/#java17-windows
3)  Verify the Java installation by checking the version from command prompt
   ![image](https://github.com/user-attachments/assets/a2c65ef1-b5b9-442c-8464-aa2428f20746)

## Step 2- Install Python
1)  Download Python 3.10.11 from https://www.python.org/downloads/release/python-31011/

## Step 3 - Install PySpark

## Step 4 - Install Apache Spark

## Step 5 - Hadoop Windows Libraries
